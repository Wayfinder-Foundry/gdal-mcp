# v1.1.0 Vector Parity & Composition - Testing Guide

**Branch:** `feat/vector-parity-v1.1`  
**Target Release:** Q1 2025  
**Focus:** Vector tool parity, cross-domain reflection, and natural composition

---

## Critical Research Questions

### Q1: Cross-Domain Reflection Cache Sharing
**Hypothesis:** CRS justification cache should work across raster and vector tools

**Test Scenario:**
```
1. User: "Reproject elevation.tif to EPSG:3857 with bilinear"
   → AI prompted for CRS justification
   → AI provides reasoning (Web Mercator for web mapping)
   → Cache stored: .preflight/justifications/crs_datum/sha256:xyz123.json

2. User: "Reproject boundaries.gpkg to EPSG:3857"
   → Middleware checks cache for dst_crs="EPSG:3857"
   → EXPECTED: Cache hit from step 1
   → EXPECTED: No re-prompting for CRS
   → AI proceeds immediately with cached reasoning

3. Verification:
   Check cache directory:
   $ ls .preflight/justifications/crs_datum/
   → Should see single SHA256 file used by both operations
```

**Success Criteria:**
- [ ] Cache hit occurs for vector operation after raster operation
- [ ] No redundant CRS prompting
- [ ] AI references previous justification naturally
- [ ] Cache file shared between both tools

**If Fails:**
- Debug cache key computation in middleware
- Verify domain consistency (`crs_datum` for both)
- Check args_fn extraction (dst_crs only)
- Examine middleware logs

---

### Q2: Natural Multi-Step Workflow Discovery
**Hypothesis:** AI can chain vector operations without explicit workflow prompts

**Test Scenario 1: Web Mapping Preparation**
```
User: "Prepare boundaries.shp for web mapping"

Expected AI workflow:
1. vector_reproject(dst_crs="EPSG:3857")
2. vector_simplify(tolerance=100, preserve_topology=true)
3. vector_convert(output="boundaries.geojson")

Measure:
- Does AI discover this chain autonomously?
- Does it ask for clarification on tolerance/simplification?
- Does it explain each step?
```

**Test Scenario 2: Format Migration**
```
User: "Migrate this old shapefile to a modern format"

Expected AI workflow:
1. vector_convert(output="data.gpkg", driver="GPKG")
2. Optional: vector_reproject if CRS standardization mentioned

Measure:
- Does AI choose GeoPackage as "modern"?
- Does it explain benefits (single file, UTF-8, no size limits)?
```

**Test Scenario 3: Spatial Subset**
```
User: "Extract features within this bounding box and prepare for analysis"

Expected AI workflow:
1. vector_clip(bounds=[...])
2. vector_reproject(dst_crs=<analysis CRS>)
3. vector_convert(output=<working format>)

Measure:
- Does AI ask for target CRS?
- Does it handle bbox input naturally?
```

**Success Criteria:**
- [ ] AI chains 2-3 operations for common tasks
- [ ] Explanations are contextual and educational
- [ ] No artificial workflow prompts needed
- [ ] Operations execute in logical order

---

### Q3: Reflection Reuse Across Operations
**Hypothesis:** Same CRS justification reused across multiple vector operations

**Test Scenario:**
```
1. User: "Reproject boundaries.gpkg to UTM Zone 10N"
   → CRS justified with rationale
   
2. User: "Also reproject roads.geojson to UTM Zone 10N"
   → EXPECTED: Cache hit, no re-prompting
   
3. User: "And the buildings.shp to UTM Zone 10N"
   → EXPECTED: Cache hit again
   
Measure:
- Cache hit rate: Should be 100% for same CRS
- AI mentions "using UTM Zone 10N (previously justified)"
- Workflow efficiency: fast execution after first justification
```

---

## Tool-Specific Tests

### vector_reproject

**Test 1: Basic Reprojection**
```bash
# Setup
Create test.geojson with simple polygon in EPSG:4326

# Test
User: "Reproject test.geojson to EPSG:3857"

Expected:
- CRS justification prompt (first use)
- Output: test_3857.gpkg (or .geojson if specified)
- Feature count preserved
- Bounds in EPSG:3857
- Geometry type preserved
```

**Test 2: Source CRS Override**
```bash
# Setup
Create shapefile without .prj file

# Test
User: "Reproject missing_crs.shp to EPSG:32610, assume source is EPSG:4326"

Expected:
- Accepts src_crs parameter
- Proceeds with override
- Warning/info about CRS assumption
```

**Test 3: Multiple Formats**
```bash
Input formats to test:
- [ ] Shapefile → GeoPackage
- [ ] GeoJSON → Shapefile
- [ ] GeoPackage → GeoJSON
- [ ] KML → GeoPackage

All should:
- Preserve attributes
- Transform geometries correctly
- Auto-detect output format from extension
```

---

### vector_convert

**Test 1: Shapefile to GeoPackage**
```bash
# Common migration scenario
Input: legacy.shp with encoding issues
Output: modern.gpkg

Expected:
- UTF-8 encoding in output
- Single file (no .shx, .dbf, .prj clutter)
- All attributes preserved
- Geometry intact
```

**Test 2: GeoJSON to Shapefile**
```bash
# Web to desktop workflow
Input: web_data.geojson
Output: desktop.shp

Expected:
- Field name truncation warnings (10 char limit)
- Encoding specification (UTF-8 vs ISO-8859-1)
- Multiple files created (.shp, .shx, .dbf, .prj)
```

**Test 3: Format Auto-Detection**
```bash
# AI should infer driver from extension
User: "Convert data.gpkg to data.geojson"

Expected:
- No explicit driver parameter needed
- GeoJSON driver selected automatically
- Pretty-printed JSON output
```

---

### vector_clip

**Test 1: Bounding Box Clip**
```bash
Input: large_dataset.gpkg
Bounds: [-123.0, 49.0, -122.0, 50.0]  # Vancouver area

Expected:
- Features outside bbox removed
- Features intersecting bbox clipped
- Partial geometries handled correctly
- Feature count < input count
```

**Test 2: Geometry Mask Clip**
```bash
Input: province_data.gpkg
Mask: city_boundary.geojson

Expected:
- Only features within city retained
- Clip respects mask geometry precisely
- Attributes preserved for retained features
```

---

### vector_buffer

**Test 1: Metric Buffer**
```bash
Input: points.geojson (EPSG:32610 - UTM Zone 10N)
Distance: 1000 (meters)

Expected:
- Circular buffers around points
- 1000m radius in projected units
- Polygons as output geometry type
```

**Test 2: Geographic Buffer**
```bash
Input: locations.geojson (EPSG:4326 - lat/lon)
Distance: 0.1 (degrees)

Expected:
- Warning about degree-based buffering?
- Buffer in degrees (approximation)
- Better: AI suggests reprojecting to metric CRS first?
```

---

### vector_simplify

**Test 1: Douglas-Peucker**
```bash
Input: detailed_coastline.gpkg (10000 vertices)
Tolerance: 100 (meters)
Method: douglas-peucker

Expected:
- Reduced vertex count (e.g., 2000 vertices)
- General shape preserved
- Smaller file size
- preserve_topology=true by default
```

**Test 2: Tolerance Impact**
```bash
Same input, different tolerances:
- 10m: minimal simplification, high detail
- 100m: moderate simplification, good for web
- 1000m: aggressive simplification, generalized

Expected:
- AI explains tradeoffs
- Suggests tolerance based on use case
- Shows before/after vertex counts
```

---

## Composition Testing

### Pattern 1: Web Mapping Workflow
```
Task: "Prepare this shapefile for web display"

Ideal sequence:
1. vector_reproject(EPSG:3857)
2. vector_simplify(tolerance=50-100)
3. vector_convert(GeoJSON or vector tiles)

Measure:
- Does AI discover this naturally?
- Are steps explained educationally?
- Does reflection work smoothly?
```

### Pattern 2: Analysis Preparation
```
Task: "Set up these datasets for spatial analysis in QGIS"

Ideal sequence:
1. vector_reproject(common CRS, e.g., UTM)
2. vector_clip(study area)
3. vector_convert(GeoPackage for single-file portability)

Measure:
- Does AI ask about target CRS?
- Does it handle multiple inputs?
- Are outputs organized logically?
```

### Pattern 3: Data Cleaning
```
Task: "Clean up this old shapefile and optimize it"

Ideal sequence:
1. vector_convert(GPKG for UTF-8 encoding)
2. vector_simplify(remove excess vertices)
3. Optional: vector_clip(remove outliers)

Measure:
- Does AI identify issues (encoding, complexity)?
- Are improvements explained?
- Is output demonstrably better?
```

---

## Session Tracking Tests

**Test: Multi-Operation Tracking**
```
Session workflow:
1. vector_reproject(EPSG:3857) 
2. vector_simplify(tolerance=100)
3. vector_convert(GeoJSON)

Expected session log (.preflight/sessions/{id}.json):
{
  "session_id": "...",
  "operations": [
    {"tool": "vector_reproject", "timestamp": "...", "params": {...}},
    {"tool": "vector_simplify", "timestamp": "...", "params": {...}},
    {"tool": "vector_convert", "timestamp": "...", "params": {...}}
  ],
  "justifications_referenced": ["crs_datum/sha256:abc123"]
}

Measure:
- Is sequence captured?
- Are timestamps accurate?
- Are justification links maintained?
```

---

## Educational Value Tests

**Test: Learning Through Doing**
```
Scenario: User new to projections

User: "Convert this file to web mapping format"
AI: "I'll reproject to EPSG:3857 (Web Mercator). This projection 
     is standard for web maps (Google Maps, Leaflet) because it 
     preserves angles and local shapes, though it distorts area 
     significantly at high latitudes."

Measure:
- Does AI explain *why* EPSG:3857?
- Is explanation contextual, not lecture?
- Can user learn projection concepts gradually?
```

**Test: Advisory Intervention**
```
Scenario: User makes suboptimal choice

User: "Reproject this global dataset to EPSG:3857 for area calculations"
AI: "I notice you want EPSG:3857 for area calculations. Web Mercator 
     significantly distorts area, especially near poles. For accurate 
     area calculations, an equal-area projection like Mollweide or 
     Albers would be better. Would you like me to suggest alternatives, 
     or proceed with Web Mercator as specified?"

Measure:
- Does AI detect concerning choices?
- Is intervention conversational, not blocking?
- Does user learn from the interaction?
```

---

## Performance Benchmarks

### Cache Hit Rate
```
Test workflow with 10 operations:
- 5 operations use EPSG:3857
- 3 operations use EPSG:32610
- 2 operations use EPSG:4326

Expected cache hit rate:
- First EPSG:3857: Miss (prompted)
- Next 4 EPSG:3857: Hit (4/4 = 100%)
- First EPSG:32610: Miss (prompted)
- Next 2 EPSG:32610: Hit (2/2 = 100%)
- Etc.

Target: >80% cache hit rate across workflow
```

### Operation Speed
```
Baseline measurements (approximate):
- vector_reproject: 1-5 seconds (depending on feature count)
- vector_convert: <1 second (format translation)
- vector_clip: 2-10 seconds (spatial operations)
- vector_buffer: 1-5 seconds (geometry generation)
- vector_simplify: 1-3 seconds (vertex reduction)

With reflection:
- First use: +10-30 seconds (LLM reasoning)
- Cache hit: +6ms (negligible)

Measure: Is cached performance acceptable?
```

---

## Failure Mode Tests

### Invalid CRS
```
User: "Reproject to EPSG:99999"

Expected:
- Clear error: "Invalid CRS specification"
- Suggestion: "Use standard formats like EPSG:3857, EPSG:4326"
- No crash, no silent failure
```

### Missing Source CRS
```
Input: Shapefile without .prj file

Expected:
- Error: "Source CRS not found and not provided"
- Instruction: "Please specify src_crs parameter"
- Example provided: src_crs="EPSG:4326"
```

### Unsupported Format
```
User: "Convert to .abc format"

Expected:
- Error: "Unsupported format: .abc"
- List supported: "Shapefile, GeoPackage, GeoJSON, KML, GML"
- Fallback to GeoPackage if extension unknown
```

---

## Success Criteria Summary

**Must Have:**
- [x] vector_reproject implemented and tested
- [ ] vector_convert implemented and tested
- [ ] Cross-domain reflection working (raster → vector cache hit)
- [ ] 3+ composition examples documented
- [ ] Cache hit rate >80% in multi-operation workflows

**Should Have:**
- [ ] vector_clip, vector_buffer, vector_simplify implemented
- [ ] Session tracking operational
- [ ] Natural workflow discovery demonstrated
- [ ] Educational value validated with user feedback

**Nice to Have:**
- [ ] Workflow guidance resources created
- [ ] Automatic pattern suggestions
- [ ] Performance optimizations
- [ ] Extended format support

---

## Testing Schedule

**Week 1:**
- [x] Implement vector_reproject
- [ ] Test cross-domain reflection (CRITICAL)
- [ ] Implement vector_convert
- [ ] Test basic format conversions

**Week 2:**
- [ ] Implement vector_clip, vector_buffer, vector_simplify
- [ ] Test composition scenarios
- [ ] Document success/failure patterns

**Week 3:**
- [ ] Session tracking implementation
- [ ] Performance benchmarking
- [ ] Educational value assessment

**Week 4:**
- [ ] Final integration testing
- [ ] Documentation updates
- [ ] Release preparation

---

## Notes

**Reflection Reuse Across Data Types:**
The key innovation in v1.1.0 is testing whether methodological reasoning transcends data formats. A CRS justified for raster analysis should apply equally to vector analysis. This requires:
- Consistent domain naming (`crs_datum`, not `raster_crs` or `vector_crs`)
- Identical cache key computation
- Same args_fn extraction
- Middleware domain-based, not tool-based

**Composition Discovery:**
We're testing whether models naturally chain operations or need explicit workflow prompts. This informs v2.x design:
- If models discover naturally → minimal scaffolding needed
- If models struggle → add workflow guidance resources
- Measure: success rate, user intervention needed, quality of results

**Educational Measurement:**
How do we know if the advisory pattern is working?
- User feedback: "I learned about projections today"
- Behavior change: Users make better choices over time
- Question patterns: Users ask "why" more often
- Confidence: Users explain their choices back to us

---

## Qualitative Assessment: Educational & Advisory Design

**Purpose:** Evaluate the educational value and advisory effectiveness of the reflection system. This is observational - record your impressions and experiences as you test.

### Observer Instructions

**As you work through tests, please note your observations on:**

#### 1. Conversational Quality
**When the AI provides CRS/method justifications:**
- Does the explanation feel natural or forced?
- Is the language accessible (not overly technical)?
- Does it feel like a helpful colleague or a lecturing professor?
- Are explanations contextual to your actual task?

**Observations to record:**
```
Example: "When I asked to reproject to EPSG:3857, the AI explained 
'Web Mercator for web mapping' naturally without technical jargon. 
Felt like a helpful colleague reminding me why this choice makes sense."
```

---

#### 2. Educational Value
**Are you learning through the process?**
- Did you discover something you didn't know?
- Did an explanation clarify a concept you were fuzzy on?
- Would you make a better choice next time because of this interaction?
- Did you learn about alternatives you weren't aware of?

**Specific areas to observe:**
- **Projections:** Do CRS explanations help you understand *why* different projections matter?
- **Resampling:** Do method descriptions clarify categorical vs continuous data handling?
- **Tradeoffs:** Are performance/quality tradeoffs explained clearly?
- **Best practices:** Does the system teach you better workflows?

**Record examples:**
```
Example: "I always used bilinear for everything. The AI's explanation 
of why nearest is better for categorical data (land cover) made me 
realize I've been corrupting my classification results. This is 
genuinely useful knowledge I'll apply going forward."
```

---

#### 3. Advisory Effectiveness (Not Blocking!)
**When you make a potentially problematic choice:**
- Does the AI catch it and ask about it?
- Is the intervention conversational (asking) vs blocking (preventing)?
- Does it feel respectful or condescending?
- Can you proceed with your choice after being informed?

**Test scenarios:**
```
Scenario 1: Request nearest neighbor for elevation data (problematic)
Observe: Does AI notice and ask about it?
Does it explain the issue (blocky artifacts, slope calculation problems)?
Can you still proceed if you want to?

Scenario 2: Request Web Mercator for area calculations (problematic)
Observe: Does AI warn about area distortion?
Does it suggest alternatives (equal-area projections)?
Can you override if you have a specific reason?

Scenario 3: Request appropriate method (e.g., cubic for elevation)
Observe: Does AI just document and proceed?
Is there unnecessary questioning of good choices?
```

**Record your impressions:**
```
Example: "I requested nearest for a DEM. The AI asked 'I notice you 
specified nearest for elevation data. This creates blocky artifacts. 
Would you like cubic instead, or proceed as specified?' This felt 
respectful - it warned me but didn't block me. I appreciated the heads-up."

Example: "I requested EPSG:5070 (Albers) for area analysis. AI just 
documented why it's appropriate and proceeded. No unnecessary questioning 
of a good choice. Perfect."
```

---

#### 4. Learning Curve Impact
**Over multiple operations:**
- Are explanations repetitive or do they adapt?
- Do you feel you're building understanding over time?
- Would a new user learn faster with this system vs traditional documentation?
- Do you eventually internalize the reasoning?

**Track progression:**
```
Session 1: "I don't really understand why projection matters"
Session 2: "Oh, area vs distance vs shape preservation - that makes sense"
Session 3: "I now automatically think about what property I need to preserve"
```

---

#### 5. Friction Points
**Where does the system get in the way?**
- Are there cases where reflection feels unnecessary?
- Times when you knew what you wanted but had to justify it anyway?
- Operations that should have been instant but required explanation?
- Redundant prompting for the same choice?

**Record friction:**
```
Example: "I used EPSG:3857 for three different layers. First time: 
justified (good). Second time: cache hit, instant (perfect). No friction."

Example: "The system asked me to justify a method I'd just used 5 minutes 
ago. Cache miss when there should have been a hit. Annoying."
```

---

#### 6. Comparison to Traditional Workflow
**How does this compare to working without reflection?**
- **Before (no AI/reflection):** How did you make these choices?
  - Trial and error?
  - Copy what you saw someone else do?
  - Read documentation then forget it?
  - Just pick defaults and hope?

- **With advisory reflection:** What's different?
  - More confident in choices?
  - Understanding why instead of just what?
  - Learning patterns you can reuse?
  - Catching mistakes earlier?

**Record comparative thoughts:**
```
Example: "Usually I just use whatever projection the first layer has. 
This system made me actually think about whether that's appropriate. 
I caught a case where my analysis CRS was wrong for the type of 
measurement I needed. Would have gotten bad results without the prompt."
```

---

### Qualitative Metrics to Track

**For each test session, provide ratings (1-5) and notes:**

#### Helpfulness
- 1 = Gets in the way, no value
- 3 = Sometimes useful, sometimes annoying
- 5 = Consistently helpful, rarely intrusive

#### Educational Value
- 1 = Learned nothing, felt lectured at
- 3 = Picked up a few things
- 5 = Significantly expanded understanding

#### Conversational Quality
- 1 = Robotic, formal, condescending
- 3 = Okay but could be more natural
- 5 = Feels like talking to a knowledgeable colleague

#### Respect for Expertise
- 1 = Questions everything, doesn't trust my choices
- 3 = Sometimes questions unnecessarily
- 5 = Trusts explicit choices, only asks when genuinely concerning

#### Overall Experience
- 1 = Frustrating, would turn off if possible
- 3 = Mixed - some good, some bad
- 5 = Genuinely makes me better at my job

---

### Open-Ended Feedback

**After completing tests, please reflect on:**

1. **Best Moment:** What was the most valuable interaction? When did the system really help you?

2. **Worst Moment:** What was the most frustrating experience? When did it get in the way?

3. **Surprise Learning:** What did you learn that you didn't expect to?

4. **Missing Piece:** What should the system explain but doesn't?

5. **Recommendation:** Would you recommend this to a colleague? Why or why not?

6. **Future Vision:** If you could add one educational feature, what would it be?

---

### Example Session Notes

**Tester:** Junie  
**Date:** 2025-10-26  
**Task:** Web mapping workflow (reproject + simplify + convert)

**Conversational Quality:** 4/5
- Explanations felt natural and contextual
- "Web Mercator for web maps" made immediate sense
- One moment felt slightly robotic (tolerance parameter explanation)

**Educational Value:** 5/5
- Learned about simplification tolerance selection (never knew how to choose)
- Understanding of projection distortion improved
- Will apply this knowledge to future projects

**Advisory Effectiveness:** 5/5
- Didn't question my appropriate choices
- Did warn when I accidentally picked nearest for continuous data
- Warning was polite and informative, not blocking

**Best Moment:**
"When I specified nearest for elevation, the AI caught it and explained 
blocky artifacts. Suggested cubic. I switched and the result was way 
better. This would have been a subtle error I might not have caught 
until seeing the final map."

**Recommendation:**
"Yes, especially for junior analysts. Senior folks might find value in 
catching edge cases they've forgotten about. The educational aspect is 
genuinely useful."

---

**Status:** Living document, updated as we test and learn
